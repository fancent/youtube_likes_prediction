{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSBD5003 Group Project - preprocessing\n",
    "\n",
    "In this part, we process the csv files as they are corrupted.\n",
    "\n",
    "1. `preprocess/preprocess.py` (no spark, pandas parallel version), `preprocess/preprocess_spark.py` (spark version)\n",
    "2. data is loaded, selected, joined (category_title), removed null, detect langugage\n",
    "3. preprocessed data is saved into `data/processed`\n",
    "\n",
    "Features:\n",
    "1. video_id\n",
    "2. title\n",
    "3. category_id\n",
    "4. tags\n",
    "5. views\n",
    "6. likes\n",
    "7. dislikes\n",
    "8. comment_count\n",
    "9. description\n",
    "10. category_title\n",
    "11. region*\n",
    "12. lang**\n",
    "\n",
    "\\* region: CA, DE, FR, GB, IN, JP, KR, MX, RU, US <br>\n",
    "** lang: see [wiki](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for lang code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc\n",
    "except:\n",
    "    print(\"That is not SparkContext. Initializing SparkContext\")\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext(\"local\", \"preprocessing\")\n",
    "try:\n",
    "    import langdetect\n",
    "except:\n",
    "    print(\"There is no langdetect, installing\")\n",
    "    !pip install langdetect\n",
    "    import langdetect\n",
    "import json\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "from time import time\n",
    "from io import StringIO\n",
    "import csv\n",
    "import numpy as np\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to read\n",
    "features = ['video_id', 'title', 'category_id', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
    "            'description']\n",
    "regions = ['CA', 'DE', 'FR', 'GB', 'IN', 'JP', 'KR', 'MX', 'RU', 'US']\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_by_line_parse(txt: str):\n",
    "    \"\"\"\n",
    "    Read the csv file line by line\n",
    "    remove the lines that are not properly formatted\n",
    "    :param txt:\n",
    "    :return: (data, number of record removed)\n",
    "    \"\"\"\n",
    "    lines = txt.split('\\n')\n",
    "    ok_lines = []\n",
    "    ok_line_indexs = []\n",
    "    fail_lines = []\n",
    "    fail_line_indexs = []\n",
    "    for i, line in enumerate(lines):\n",
    "        f = StringIO(line)\n",
    "        c = list(csv.reader(f))\n",
    "        if len(c) > 1:\n",
    "            raise ValueError('Number of lines of a line > 1')\n",
    "        elif len(c) <= 0:\n",
    "            fail_lines.append(c)\n",
    "            fail_line_indexs.append(i)\n",
    "            continue\n",
    "        c = c[0]\n",
    "        if len(c) == 16:\n",
    "            ok_lines.append(c)\n",
    "            ok_line_indexs.append(i)\n",
    "        else:\n",
    "            fail_lines.append(c)\n",
    "            fail_line_indexs.append(i)\n",
    "    fail_line_indexs = np.array(fail_line_indexs)\n",
    "    invalid_line_indexs = [i[0] - 1 for i in\n",
    "                           np.split(fail_line_indexs, np.where(np.diff(fail_line_indexs) != 1)[0] + 1)]\n",
    "    for _ind in invalid_line_indexs:\n",
    "        ind = ok_line_indexs.index(_ind)\n",
    "        ok_line_indexs.pop(ind)\n",
    "        ok_lines.pop(ind)\n",
    "    data = pd.DataFrame(ok_lines[1:], columns=ok_lines[0])\n",
    "    return data, len(invalid_line_indexs)\n",
    "\n",
    "\n",
    "def drop_invalid_byte(buf, offset=0, calls=0) -> tuple:\n",
    "    \"\"\"\n",
    "\n",
    "    :param buf:\n",
    "    :param offset:\n",
    "    :param calls:\n",
    "    :return: text, number of byte removed,\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    end = len(buf)\n",
    "    try:\n",
    "        # print(f\"Extracting from {start+offset}:{end+offset}\")\n",
    "        temp = buf[start:end].decode('utf-8-sig')\n",
    "        return temp, calls\n",
    "    except UnicodeDecodeError as err:\n",
    "        ERR = err\n",
    "        # print(ERR)\n",
    "        if ERR.reason == 'invalid continuation byte':\n",
    "            tailstr, tail_calls = drop_invalid_byte(buf[ERR.end + 1: end], ERR.end + 1)\n",
    "            return buf[start:ERR.start].decode('utf-8-sig') + tailstr, calls + 1 + tail_calls\n",
    "        elif ERR.reason == 'invalid start byte':\n",
    "            return buf[start + 1: end], calls + 1\n",
    "        elif ERR.reason == 'unexpected end of data':\n",
    "            return buf[start: end - 1], calls + 1\n",
    "    except Exception as err:\n",
    "        raise err\n",
    "\n",
    "\n",
    "def safe_read_csv(csv_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(csv_name, encoding='utf-8-sig')\n",
    "    except UnicodeDecodeError:\n",
    "        f = open(csv_name, 'rb')\n",
    "        raw_buffer = f.read()\n",
    "        f.close()\n",
    "        txt, nbytes = drop_invalid_byte(raw_buffer)\n",
    "        print(f\"Removed {nbytes} invalid bytes\")\n",
    "        f = StringIO(txt)\n",
    "        try:\n",
    "            return pd.read_csv(f)\n",
    "        except:\n",
    "            data, num_of_invalid_lines = line_by_line_parse(txt)\n",
    "            print(f\"Removed {num_of_invalid_lines} records in {csv_name}\")\n",
    "            return data\n",
    "\n",
    "def detect_language(x):\n",
    "    import langdetect\n",
    "    try:\n",
    "        return langdetect.detect(x)\n",
    "    except:\n",
    "        return ''\n",
    "detect_language_udf = udf(detect_language, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start process CA\n",
      "Finished process CA (with 40807 records) in 164.13s\n",
      "Start process DE\n",
      "Finished process DE (with 40584 records) in 145.11s\n",
      "Start process FR\n",
      "Finished process FR (with 40610 records) in 170.29s\n",
      "Start process GB\n",
      "Finished process GB (with 38826 records) in 197.18s\n",
      "Start process IN\n",
      "Finished process IN (with 37247 records) in 192.06s\n",
      "Start process JP\n",
      "Removed 9 invalid bytes\n",
      "Finished process JP (with 20505 records) in 32.13s\n",
      "Start process KR\n",
      "Removed 66 invalid bytes\n",
      "Removed 1 records in ../data/raw/KRvideos.csv\n",
      "Finished process KR (with 34279 records) in 43.76s\n",
      "Start process MX\n",
      "Removed 3 invalid bytes\n",
      "Finished process MX (with 40197 records) in 197.89s\n",
      "Start process RU\n",
      "Removed 43 invalid bytes\n",
      "Removed 9 records in ../data/raw/RUvideos.csv\n",
      "Finished process RU (with 39183 records) in 119.50s\n",
      "Start process US\n",
      "Finished process US (with 40949 records) in 122.16s\n"
     ]
    }
   ],
   "source": [
    "for region in regions:\n",
    "    if os.path.isfile(f'../data/processed/spark/{region}.csv'):\n",
    "        os.remove(f'../data/processed/spark/{region}.csv')\n",
    "    elif os.path.isdir(f'../data/processed/spark/{region}.csv'):\n",
    "        shutil.rmtree(f'../data/processed/spark/{region}.csv')\n",
    "    print(f\"Start process {region}\")\n",
    "    start_time = time()\n",
    "    csv_name = f'../data/raw/{region}videos.csv'\n",
    "    json_name = f'../data/raw/{region}_category_id.json'\n",
    "    data = safe_read_csv(csv_name)[features].astype(\n",
    "                    {'video_id': str, 'title': str, 'category_id': int, 'tags': str, 'views': int, 'likes': int,\n",
    "                     'dislikes': int, 'comment_count': int, 'description': str})\n",
    "    category_data = pd.DataFrame(list(\n",
    "                    map(lambda x: {'category_id': int(x['id']), 'category_title': x['snippet']['title']},\n",
    "                        json.load(open(json_name, 'r'))['items'])))\n",
    "    data_sp = spark.createDataFrame(data)\n",
    "    cat_data_sp = spark.createDataFrame(category_data)\n",
    "    data_sp = data_sp.join(cat_data_sp, 'category_id')\n",
    "    data_sp = data_sp.withColumn('region', lit(region)).cache()\n",
    "    data_sp = data_sp.select('*', detect_language_udf(data_sp['title']).alias('lang'))\n",
    "    data_sp.write.csv(f'../data/processed/spark/{region}.csv')\n",
    "    print(f\"Finished process {region} (with {data_sp.count()} records) in {time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
